<!DOCTYPE html>
<html>
  <head>
    <title>Time Series, Data Processing and Delivery (ETL and API)</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
    </style>
    <link rel="stylesheet" href="../style.css">
  </head>
  <body>
    <textarea id="source">

class: center, middle

Elements of Data Science - F2020

# Time Serices, Data Processing and Delivery (ETL and API)

04/27/2020

---
# Time Series

--
count:false
### Data ordered in time
--
count:false
- Applications
--
count:false
    - Financial
--
count:false
    - Economic
--
count:false
    - Scientific
--
count:false
    - etc.

---
# Time Series Differences

--
count:false
- Non-i.i.d.!
--
count:false
- not independent
    - Ex: Stock price
--
count:false
- not-identically distributed
    - Ex: Seasonality
--
count:false
- Order matters

---
# Representing Time in Python

--
count:false
- `datetime` library

--
count:false
- Pandas `Timestamp`

---
# `datetime.date`

```python
from datetime import date
```

--
count:false
```python
friday = date(2019,11,22) # year,month,day
friday
```
```
datetime.date(2019, 11, 22)
```

--
count:false
```python
today = date.today()
today
```
```
datetime.date(2019, 11, 22)
```

--
count:false
```python
today.year
```
```
2019
```



---
# `datetime.time`

```python
from datetime import time
```
--
count:false
```python
noon = time(12,0,0) # hour,minute,second,microsecond 
noon
```
```
datetime.time(12, 0)
```

--
count:false
```python
noon.hour
```
```
12
```

---
# `datetime.datetime`

```python
from datetime import datetime
```

--
count:false
```python
# year,month,day,hour,minute,second,microsecond
friday_afternoon = datetime(2019,11,22,15,30)
friday_afternoon
```
```
datetime.datetime(2019, 11, 22, 15, 30)
```

--
count:false
```python
now = datetime.now()
now
```
```
datetime.datetime(2019, 11, 22, 14, 36, 4, 104824)
```

---
# `datetime.timedelta`

```python
diff = datetime(2019,11,22,1) - datetime(2019,11,22)
diff
```
```
datetime.timedelta(days=1, seconds=3600)
```

--
count:false
```python
diff.total_seconds()
```
```
90000.0
```

--
count:false
```python
from datetime import timedelta

#days,seconds,microseconds,milliseconds,minutes,hours,weeks
one_day = timedelta(1)

date(2019,11,22) + 2*one_day
```
```
datetime.date(2019, 11, 24)
```

---
# Printing Datetimes: `strftime`

```python
print(now)
```
```
2020-04-27 14:08:35.831394

```
--
count:false
```python
now.strftime('%a %h %d, %Y %I:%M %p')
```
```
'Mon Apr 27, 2020 02:08 PM'
```
--
count:false

```
%Y 4-digit year
%y 2-digit year
%m 2-digit month
%d 2-digit day
%H Hour (24-hour)
%M 2-digit minute
%S 2-digit second
```

See [strftime.org](http://strftime.org/)


---
# Parsing Datetimes: to_datetime

--
count:false
- `dateutil.parser` available
--
count:false
- pandas has parser built in: `pd.to_datetime()`

--
count:false
.smallest[
```python
import pandas as pd

pd.to_datetime('11/22/2019 2:36pm')
```]
.smallest[
```
Timestamp('2019-11-22 14:36:00')
```]

--
count:false
.smallest[
```python
dt_index = pd.to_datetime([datetime(2019, 11, 26),
                           '27th of November, 2019',
                           '2019-Nov-28',
                           '11-29-2019',
                           '20191201',
                           None
                          ])
dt_index
```
```
DatetimeIndex(['2019-11-26', '2019-11-27', '2019-11-28', '2019-11-29',
               '2019-12-01', 'NaT'], dtype='datetime64[ns]', freq=None)
```]


---
# `pandas.Timestamp`

--
count:false
- like `datetime.datetime`
--
count:false
- can include **timezone** and **frequency** info
--
count:false
- can be used anywhere datetime can be used
--
count:false
- can handle a missing time: `NaT`
--
count:false
- an array of Timestamps can be used as an index

--
count:false
```python
dt_index[0]

```
```
Timestamp('2019-11-26 00:00:00')
```

---
# DateIndex Indexing/Selecting/Slicing

--
count:false
.smaller[
```python
s = pd.Series([101,102,103],
              index=pd.to_datetime(['20191201','20200101','20200201']))
s
```]
```
2019-12-01    101
2020-01-01    102
2020-02-01    103
dtype: int64
```

--
count:false
```python
s.iloc[0:2]
```
```
2019-12-01    101
2020-01-01    102
dtype: int64
```

---
# DateIndex Indexing/Selecting/Slicing
--
count:false
```python
s['2020']
```
```
2020-01-01    102
2020-02-01    103
dtype: int64
```

--
count:false
```python
s['2020-01']
```
```
2020-01-01    102
dtype: int64
```

--
count:false
```python
s['01/01/2019':'01/01/2020']
```
```
2019-12-01    101
2020-01-01    102
dtype: int64
```

---
# Datetimes in DataFrames

.smaller[
```python
df = pd.DataFrame([['12/1/2019',101,'A'],
                   ['1/1/2020',102,'B']],columns=['col1','col2','col3'])
df.col1 = pd.to_datetime(df.col1)
df.set_index('col1',drop=True,inplace=True)
```
```
            col2 col3
col1
2019-12-01   101    A
2020-01-01   102    B
```]

--
count:false
```python
df.loc['2020']
```
```
            col2 col3
col1
2020-01-01   102    B
```

---
# Setting Frequency

--
count:false
.smallest[
```python
s = pd.Series([101,103],index=pd.to_datetime(['20191201','20191203']))
s
```
```
2019-12-01    101
2019-12-03    103
dtype: int64
```]

--
count:false
.smallest[
```python
s.resample('D').asfreq()
```
```
2019-12-01    101.0
2019-12-02      NaN
2019-12-03    102.0
Freq: D, dtype: float64
```]

--
count:false
.smallest[
```python
pd.to_datetime(['20191201','20191203'])
```
```
DatetimeIndex(['2019-12-01', '2019-12-03'], dtype='datetime64[ns]', freq=None)
```]

--
count:false
.smallest[
```python
pd.date_range(start='20191201',end='20191203',freq='D')
```
```
DatetimeIndex(['2019-12-01', '2019-12-02', '2019-12-03'], dtype='datetime64[ns]', freq='D')
```]

---
## Sample of Available Frequencies


.smallest[
```
Alias	Description
___________________________________
B    business day frequency
C    custom business day frequency
D    calendar day frequency
W    weekly frequency
M    month end frequency
SM   semi-month end frequency (15th and end of month)
BM   business month end frequency
MS   month start frequency
...
Q    quarter end frequency
BQ   business quarter end frequency
...
Y    year end frequency
BY   business year end frequency
...
BH      business hour frequency
H       hourly frequency
T,min   minutely frequency
S       secondly frequency
L,ms    milliseconds
U,us    microseconds
N       nanoseconds
```]

.tiny[
https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases]

---
# Timezones

- Handled by `pytz`

--
count:false
```python
import pytz
[x for x in pytz.common_timezones if x.startswith('U')]
```
```
['US/Alaska',
 'US/Arizona',
 'US/Central',
 'US/Eastern',
 'US/Hawaii',
 'US/Mountain',
 'US/Pacific',
 'UTC']
```

--
count:false
**UTC**: coordinated universal time (EST is 5 hours behind, -5:00)

---
# Timezones

--
count:false
.smaller[
```python
ts = pd.date_range('11/2/2019 9:30am',periods=2,freq='D')
ts
```
```
DatetimeIndex(['2019-11-02 09:30:00',
               '2019-11-03 09:30:00'], dtype='datetime64[ns]', freq='D')
```]

--
count:false
.smaller[
```python
ts_utc = ts.tz_localize('UTC')
ts_utc
```
```
DatetimeIndex(['2019-11-02 09:30:00+00:00',
               '2019-11-03 09:30:00+00:00'], dtype='datetime64[ns, UTC]', freq='D')
```]

--
count:false
.smaller[
```python
ts_utc.tz_convert('US/Eastern')
```
```
DatetimeIndex(['2019-11-02 05:30:00-04:00',
               '2019-11-03 04:30:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq='D')
```]

---
## Timeseries in Python so far:

- datetime .date .time .datetime .timedelta
- format with .strftime()
- parse time with pd.to_datetime()
- panda Timestamp Timedelta DatetimeIndex
- Indexing with DatetimeIndex
- Frequencies
- Timezones

--
count:false
## Didn't discuss:
- Period and PeriodIndex
- Panels

--
count:false
--
count:false
## Next: Operations on Time Series data


---
# Shifting

- Moving data backward or forward in time (lagging/leading)
- Ex: percent change

--
count:false
.smaller[
```python
ts = pd.Series([1,2,8],
               index=pd.date_range('1/1/2019',periods=3,freq='M'))
ts
```
```
2019-01-31    1
2019-02-28    2
2019-03-31    8
Freq: M, dtype: int64
```]

--
count:false
.smaller[
```python
ts.shift(1) # old value
```
```
2019-01-31    NaN
2019-02-28    1.0
2019-03-31    2.0
Freq: M, dtype: float64
```]

---
# Shifting

- **percent change**: `(new_value - old_value) / old_value`
- **percent change**: `(new_value / old_value) - 1`

--
count:false
```python
#  new - old / old == (new/old) - 1
(ts / ts.shift(1)) - 1
```
```
2019-01-31    NaN
2019-02-28    1.0
2019-03-31    3.0
Freq: M, dtype: float64
```
---
# Example Dataset: Twitter Stock

.smallest[
```python
#from pandas_datareader import data
#twtr = data.DataReader('TWTR', start='2013', end='2018', data_source='yahoo') 
twtr = pd.read_csv('../data/twtr_2013-2018.csv',parse_dates=['Date'])
twtr = twtr.set_index('Date')
```
```
                 High        Low       Open      Close     Volume  Adj Close
Date
2013-11-07  50.090000  44.000000  45.099998  44.900002  117701600  44.900002
2013-11-08  46.939999  40.689999  45.930000  41.650002   27925300  41.650002
2013-11-11  43.000000  39.400002  40.500000  42.900002   16113900  42.900002
2013-11-12  43.779999  41.830002  43.660000  41.900002    6316700  41.900002
2013-11-13  42.869999  40.759998  41.029999  42.599998    8688300  42.599998
```]

.center[
![:scale 50%](images/twitter_close.png)]


---
# Example Dataset: Twitter Stock

- Percent Change

.smallest[
```python
(twtr / twtr.shift(1) - 1).head()
```
```
                High       Low      Open     Close    Volume  Adj Close
Date                                                                   
2013-11-07       NaN       NaN       NaN       NaN       NaN        NaN
2013-11-08 -0.062887 -0.075227  0.018404 -0.072383 -0.762745  -0.072383
2013-11-11 -0.083937 -0.031703 -0.118223  0.030012 -0.422964   0.030012
2013-11-12  0.018140  0.061675  0.078025 -0.023310 -0.607997  -0.023310
2013-11-13 -0.020786 -0.025580 -0.060238  0.016706  0.375449   0.016706
```]

---
# Resampling

--
count:false
- Convert from one frequency to another

--
count:false
- **Downsampling**
    - from higher to lower (day to month)
    - need to aggregate

--
count:false
- **Upsampling**
    - from lower to higher (month to day)
    - need to fill missing


--
count:false
- Can also be used to set frequency from None

---
# Resampling: Initialize Frequency

.smallest[
```python
twtr.index
```
```
DatetimeIndex(['2013-11-07', '2013-11-08', '2013-11-11', '2013-11-12',
               ...
               '2018-11-08', '2018-11-09'],
*             dtype='datetime64[ns]', name='Date', length=1262, freq=None)
```]

--
count:false
.smallest[
```python
twtr_B = twtr.resample('B').asfreq() # set frequncy to business day
```]

--
count:false
.smallest[
```python
twtr_B.index
```
```
DatetimeIndex(['2013-11-07', '2013-11-08', '2013-11-11', '2013-11-12',
               ...
               '2018-11-08', '2018-11-09'],
*             dtype='datetime64[ns]', name='Date', length=1307, freq='B')
```]

---
# Resampling: Downsampling

- Go from shorter to longer
--
count:false
- Need to aggregate (like groupby)
--
count:false
- Example: Downsampling from day business quarter

--
count:false
.smallest[
```python
twtr_BQ = twtr.resample('BQ')
print(twtr_BQ)
```
```
DatetimeIndexResampler [freq=&lt;BusinessQuarterEnd: startingMonth=12&gt;,
                        axis=0, closed=right, label=right,
                        convention=start, base=0]
```]
--
count:false
.smallest[
```python
twtr_BQ.mean().head(3)
```
```
                 High        Low       Open      Close        Volume  Adj Close
Date
2013-12-31  51.061892  47.848108  49.208919  49.657568  2.285179e+07  49.657568
2014-03-31  57.966558  55.557705  56.952459  56.587049  1.584258e+07  56.587049
2014-06-30  38.741905  37.008254  37.923651  37.854603  2.675303e+07  37.854603

```]

---
# Resampling: Downsampling

.smallest[
```python
fig = plt.figure(figsize=(12,6))
twtr.Close.plot(style='-', label='by D')
twtr_BQ.Close.mean().plot(style='--',label='by BQ')
plt.legend(loc='upper right');
```]
.center[
![](images/twtr_downsample.png)]

---
# Resampling: Upsampling

--
count:false
- Go from longer to shorter
--
count:false
- Need to decide how to handle missing values
--
count:false
- Example: Upsample from day to hour

--
count:false
.smallest[
```python
twtr.Close.resample('H').asfreq().head()
```
```
Date
2013-11-07 00:00:00    44.900002
2013-11-07 01:00:00          NaN
2013-11-07 02:00:00          NaN
Freq: H, Name: Close, dtype: float64
```]

---
# Resampling: Upsampling

--
count:false
- `ffill`: Forward Fill 

--
count:false
.smallest[
```python
twtr.Close.resample('H').ffill().head(3)
```
```
Date
2013-11-07 00:00:00    44.900002
2013-11-07 01:00:00    44.900002
2013-11-07 02:00:00    44.900002
Freq: H, Name: Close, dtype: float64
```]

--
count:false
- `bfill`: Backward Fill 

--
count:false
.smallest[
```python
twtr.Close.resample('H').bfill().head(3)
```
```
Date
2013-11-07 00:00:00    44.900002
2013-11-07 01:00:00    41.650002
2013-11-07 02:00:00    41.650002
Freq: H, Name: Close, dtype: float64
```]

---
# Moving Windows

--
count:false
- Apply function on a fixed window moving accross time
--
count:false
- **Center**: place values at center of window

--
count:false
.smaller[
```python
rolling = twtr.Close.rolling(30, center=True)
rolling
```
```
Rolling [window=30,center=True,axis=0]
```]

--
count:false
.smaller[
```python
rolling.mean()['2013-11-25':'2013-12-3']
```
```
Date
2013-11-25          NaN
2013-11-26          NaN
2013-11-27          NaN
2013-11-29    46.053334
2013-12-02    46.557000
2013-12-03    47.320000
Name: Close, dtype: float64
```]

---
# Moving Windows

.smallest[
```python
fig,ax = plt.subplots(1,1,figsize=(12,6));
twtr.Close.plot(style='-',alpha=0.3);
rolling.mean().plot(style='--');
(rolling.mean() + 2*rolling.std()).plot(style=':',c='g');
(rolling.mean() - 2*rolling.std()).plot(style=':',c='g');
```]
.center[
![:scale 80%](images/twtr_rolling.png)]


---
# Example: Bike Travel

.smallest[
- From PDSH Chapter 3.11]

--
count:false
.smallest[
```sh
# Gather data of bike activity on Fremont Bridge bike traffic
!curl -o ../data/FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 1934k    0 1934k    0     0  1170k      0 --:--:--  0:00:01 --:--:-- 1169k
```]

--
count:false
.smallest[
```python
df = pd.read_csv('../data/FremontBridge.csv', index_col='Date', parse_dates=True)
df = df.loc['2015':'2017']
df.columns = ['Total','East','West']
df.info()
```
```
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 26304 entries, 2015-01-01 00:00:00 to 2017-12-31 23:00:00
Data columns (total 3 columns):
Total    26299 non-null float64
East     26299 non-null float64
West     26299 non-null float64
dtypes: float64(3)
memory usage: 822.0 KB
```]

---
# Example: Bike Travel

```python
f'proportion missing: {sum(df.Total.isna()) / len(df):0.5f}'
```
```
'proportion missing: 0.00019'
```

--
count:false
```python
df = df.fillna(method='ffill')
print(df.head(3))
```
```
                     Total  East  West
Date
2015-01-01 00:00:00   13.0   4.0   9.0
2015-01-01 01:00:00   27.0   4.0  23.0
2015-01-01 02:00:00   19.0   5.0  14.0
```

---
# Example: Bike Travel

.smallest[
```python
# plot data from 2015
df['2015'].plot(style=['-', '--', ':'],figsize=(12,6))
plt.ylabel('Hourly Bicycle Count');
```]
.center[
![](images/bike_hourly.png)]


---
# Example: Bike Travel

.smallest[
```python
# downsample to weekly sum to smooth things out
weekly = df.resample('W').sum()
weekly.plot(style=[':', '--', '-'], figsize=(12,6))
plt.ylabel('Weekly bicycle count');
```]
.center[
![](images/bike_weekly.png)]

---
# Example: Bike Travel

.smallest[
```python
# resample at daily for a more granular view then apply a rolling window of 30 days to smooth things out
daily = df.resample('D').sum()
daily.rolling(30,center=True).mean().plot(style=[':', '--', '-'], figsize=(12,6))
plt.ylabel('mean daily count');
```]
.center[
![](images/bike_daily_rolling.png)]

---
# Example: Bike Travel

.smallest[
```python
# a wider window using a gaussian filter smooths more while accentuating daily differences
daily.rolling(50,center=True,win_type='gaussian').mean(std=30).plot(style=[':','--','-'],figsize=(12,6));
plt.ylabel('mean daily count');
```]
.center[
![](images/bike_daily_rolling_gaussian.png)]

---
# Example: Bike Travel

- If we want to only look at time of day

--
count:false
.smaller[
```python
data.index.time
```
```
array([datetime.time(0, 0), datetime.time(1, 0), datetime.time(2, 0), ...,
       datetime.time(21, 0), datetime.time(22, 0), datetime.time(23, 0)],
      dtype=object)
```]

--
count:false
- Get mean data by time (hourly)

--
count:false
.smaller[
```python
by_time = df.groupby(df.index.time).mean()
print(by_time.head(3))
```
```
              Total      East      West
00:00:00  11.319343  4.806569  6.512774
01:00:00   5.743613  2.613139  3.130474
02:00:00   3.615876  1.687956  1.927920
```]

---
# Example: Bike Travel

.smallest[
```python
# create xticks every 4 hours
hourly_ticks = 60 * 60 * 4 * np.arange(6)  # sec * min * every4hours
by_time.plot(xticks=hourly_ticks, style=[':', '--', '-'], figsize=(12,6));
plt.ylabel('mean hourly count')
```]
.center[
![](images/bike_hourly.png)]

---
# Example: Bike Travel

.smallest[
```python
# can also look at average by day of week
by_weekday = df.groupby(df.index.dayofweek).mean()

fig,ax = plt.subplots(1,1,figsize=(12,6))
by_weekday.plot(style=[':', '--', '-'], ax=ax);
ax.set_xlabel('Day of Week');ax.set_ylabel('mean daily count')
ax.set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']);
```]
.center[
![:scale 70%](images/bike_weekday.png)]

---
# Example: Bike Travel

.smallest[
```python
# create a weekend mask
weekend = np.where(df.index.weekday &lt; 5, 'Weekday', 'Weekend')

# get hourly mean values split by weekday, weekend
by_time = df.groupby([weekend, df.index.time]).mean()

fig, ax = plt.subplots(1, 2, figsize=(14, 5))
by_time.loc['Weekday'].plot(ax=ax[0], title='Weekdays', xticks=hourly_ticks, style=[':', '--', '-'])
by_time.loc['Weekend'].plot(ax=ax[1], title='Weekends', xticks=hourly_ticks, style=[':', '--', '-']);
```]
.center[
![:scale 80%](images/bike_weekday_weekend.png)]


---
# Example: Bike Travel

- Can we predict daily Total bike traffic?

--
count:false
.smallest[
```python
bike_counts = pd.read_csv('../data/FremontBridge.csv', index_col='Date', parse_dates=True)
bike_weather = pd.read_csv('../data/BicycleWeather.csv', index_col='DATE', parse_dates=True)
```]

--
count:false
.smallest[
```python
# define target
daily = bike_counts.resample('d').sum()
daily['Total'] = daily.sum(axis=1)
daily = daily[['Total']] # remove other columns
print(daily.head(3))
```
```
             Total
Date
2012-10-03  7042.0
2012-10-04  6950.0
2012-10-05  6296.0
```]

---
# Example: Bike Travel

.smallest[
```python
# add 'day of week' one-hot features
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
for i in range(7):
    daily[days[i]] = (daily.index.dayofweek == i).astype(float)
print(daily.head(3))
```
```
             Total  Mon  Tue  Wed  Thu  Fri  Sat  Sun
Date
2012-10-03  7042.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0
2012-10-04  6950.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0
2012-10-05  6296.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0
```]

---
# Example: Bike Travel

.smallest[
```python
# add 'is it a holiday' dummy feature
from pandas.tseries.holiday import USFederalHolidayCalendar
cal = USFederalHolidayCalendar()
holidays = cal.holidays('2012', '2016')
daily = daily.join(pd.Series(1, index=holidays, name='holiday'))
daily['holiday'].fillna(0, inplace=True)
print(daily.head(3))
```
```
             Total  Mon  Tue  Wed  Thu  Fri  Sat  Sun  holiday
Date
2012-10-03  7042.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0      0.0
2012-10-04  6950.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0      0.0
2012-10-05  6296.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0      0.0
```]

---
# Example: Bike Travel

.smallest[
```python
# add number of hours of daylight
def hours_of_daylight(date, axis=23.44, latitude=47.61):
    """Compute the hours of daylight for the given date"""
    days = (date - pd.datetime(2000, 12, 21)).days
    m = (1. - np.tan(np.radians(latitude))
         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))
    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.

daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index));
daily[['daylight_hrs']].plot();
plt.ylim(8, 17);
```]
.center[
![](images/bike_daylight.png)]

---
# Example: Bike Travel

- Add weather information (can we predict this for future dates?)

.smallest[
```python
# temperatures are in 1/10 deg C; convert to C
bike_weather['TMIN'] /= 10
bike_weather['TMAX'] /= 10
bike_weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])

# precip is in 1/10 mm; convert to inches
bike_weather['PRCP'] /= 254
bike_weather['dry day'] = (bike_weather['PRCP'] == 0).astype(int)

daily = daily.join(bike_weather[['PRCP', 'Temp (C)', 'dry day']])
print(daily.head(3))
```
```
             Total  Mon  Tue  Wed  Thu  Fri  Sat  Sun  holiday  daylight_hrs  PRCP  Temp (C)  dry day
Date
2012-10-03  7042.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0      0.0     11.277359   0.0     13.35      1.0
2012-10-04  6950.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0      0.0     11.219142   0.0     13.60      1.0
2012-10-05  6296.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0      0.0     11.161038   0.0     15.30      1.0
```]

---
# Example: Bike Travel


.smallest[
```python
# add how far into the year we are?
daily['annual'] = (daily.index - daily.index[0]).days / 365.
daily.head(3)
```
```
             Total  Mon  Tue  Wed  Thu  Fri  Sat  Sun  holiday  daylight_hrs  PRCP  Temp (C)  dry day    annual
Date
2012-10-03  7042.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0      0.0     11.277359   0.0     13.35      1.0  0.000000
2012-10-04  6950.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0      0.0     11.219142   0.0     13.60      1.0  0.002740
2012-10-05  6296.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0      0.0     11.161038   0.0     15.30      1.0  0.005479
```
]

---
# Example: Bike Travel
.smallest[
```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

daily.dropna(axis=0, how='any', inplace=True)

X = daily[daily.columns[daily.columns != 'Total']]
y = daily.Total
X_train,X_test,y_train,y_test = train_test_split(X,y)

model = LinearRegression(fit_intercept=False)
model.fit(X_train,y_train)
model.score(X_test,y_test)
```
```
0.8490988503212465
```]


---
# Example: Bike Travel

.smallest[
```python
daily['predicted'] = model.predict(X)
daily[['Total', 'predicted']].rolling(30, center=True).mean().plot(alpha=0.5,figsize=(12,6))
plt.tight_layout()
```]
.center[
![](images/bike_predictedvstotal.png)]

---
# Time Series Review

- Shifting
- Resampling
    - Downsampling
    - Upsampling
- Moving Windows

---
class:middle

# Questions?

---
# ETL 

--
count:false
- Extract - extract data from source
--
count:false
- Transform - clean, verify, engineer
--
count:false
- Load - combine, store

---
# E-Step: Extract

--
count:false
- flatfiles (csv, excel)
--
count:false
- semi-structured documents (json, html)
--
count:false
- unstructured documents
--
count:false
- data + schema (dataframe, parquet)
--
count:false
- APIs (wikipedia, twitter, spotify, etc.)
--
count:false
- databases

<br>
--
count:false
- Pandas to the rescue!
--
count:false
- Plus other specialized libraries

---
# Reading Data with Pandas

- read_csv
- read_excel
- read_table
- read_excel
- read_json
- read_html
- read_parquet
- read_sql
- read_clipboard
- ...

---
# CSV

**Comma Separated Values**


```
Year,Make,Model,Description,Price
1997,Ford,E350,"ac, abs, moon",3000.00
1999,Chevy,"Venture ""Extended Edition""","",4900.00
1999,Chevy,"Venture ""Extended Edition, Very Large""",,5000.00
1996,Jeep,Grand Cherokee,"MUST SELL!
air, moon roof, loaded",4799.00
```
<br>
.smallest[
```python
df = pd.read_csv('../data/example.csv',header=1,sep=',')
```
```
   Year   Make                                   Model                         Description   Price
0  1997   Ford                                    E350                       ac, abs, moon  3000.0
1  1999  Chevy              Venture "Extended Edition"                                 NaN  4900.0
2  1999  Chevy  Venture "Extended Edition, Very Large"                                 NaN  5000.0
3  1996   Jeep                          Grand Cherokee  MUST SELL!\nair, moon roof, loaded  4799.0
```]

---
# CSV

```python
# just a subset of options
pandas.read_csv(filepath_or_buffer,
    sep=',',        # usually comma, often tab
    header='infer',
    index_col=None,
    dtype=None,
    skiprows=None,
    na_values=None,
    skip_blank_lines=True,
    parse_dates=False,
    infer_datetime_format=False,
    compression='infer',
    quotechar='"',
    encoding=None   # usually utf-8 or latin_1
```
.tiny[
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv]

---
# JSON

.smaller[
**JavaScript Object Notation**
- often seen as return from api call
- pretty print using `json.loads(json_string)`
]

.smallest[
```
{
  "firstName": "John",
  "lastName": "Smith",
  "isAlive": true,
  "age": 27,
  "phoneNumbers": [
    {
      "type": "home",
      "number": "212 555-1234"
    },
    {
      "type": "office",
      "number": "646 555-4567"
    },
  ],
  "children": [],
  "spouse": null
}
```]

---
# JSON

.smaller[
- pandas requires length of arrays of same length
]

.smallest[
```
{
 '0': {'Year': 1997,
  'Make': 'Ford',
  'Model': 'E350',
  'Description': 'ac, abs, moon',
  'Price': 3000.0},
 '1': {'Year': 1999,
  'Make': 'Chevy',
  'Model': 'Venture "Extended Edition"',
  'Description': None,
  'Price': 4900.0},
 '2': {'Year': 1999,
  'Make': 'Chevy',
  'Model': 'Venture "Extended Edition, Very Large"',
  'Description': None,
  'Price': 5000.0},
 '3': {'Year': 1996,
  'Make': 'Jeep',
  'Model': 'Grand Cherokee',
  'Description': 'MUST SELL!\nair, moon roof, loaded',
  'Price': 4799.0}
}
```]

---
# HTML

.smaller[
**HyperText Markup Language**
- Parse with BeautifulSoup
]

.smallest[
```html
<html>
    <head>
        <title>Example</title>
    </head>
    <body>
        <p id="first" class="example"><strong>Example text!</strong></p>
        <p id="second" class="example">And More!</p>
    </body>
</html>
```]

.smallest[
```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html)

[p.text for p in soup('p')]
```
```
['Example text!', 'And More!']
```]

---
# Parquet

- open source column-oriented data storage
- part of the Apache Hadoop ecosystem
- often used when working with Spark
- requires additional parsing engine eg `pyarrow`
- includes both data and schema


- **Schema**: metadata about the dataset (column names, datatypes, etc.)

---
# Getting Data: APIs

**Application Programming Interface**
- set of definitions of how to request and recieve data from a datasource
- most datasources have an API
- some require authentication
- python libraries for most common APIs


- `requests`: library for making web requests and accessing the results 

---
# Example: Wikipedia

.smallest[
```python
import requests
url = 'http://en.wikipedia.org/w/api.php?action=query&prop=info&format=json&titles='
title = 'Data Science'
title = title.replace(' ','%20')
print(url+title)
```
```
http://en.wikipedia.org/w/api.php?action=query&prop=info&format=json&titles=Data%20Science
```]

.smallest[
```python
resp = requests.get(url+title)
resp.json()
```
```
{'batchcomplete': '',
 'query': {'pages': {'49495124': {'pageid': 49495124,
    'ns': 0,
    'title': 'Data Science',
    'contentmodel': 'wikitext',
    'pagelanguage': 'en',
    'pagelanguagehtmlcode': 'en',
    'pagelanguagedir': 'ltr',
    'touched': '2019-11-22T01:43:30Z',
    'lastrevid': 706007296,
    'length': 26,
    'redirect': '',
    'new': ''}}}}
```]

---
# Example: Twitter

.smaller[
```python
with open('/home/bgibson/proj/twitter/twitter_consumer_key.txt') as f:
    consumer_key = f.read().strip()
with open('/home/bgibson/proj/twitter/twitter_consumer_secret.txt') as f:
    consumer_secret = f.read().strip()
with open('/home/bgibson/proj/twitter/twitter_access_token.txt') as f:
    access_token = f.read().strip()
with open('/home/bgibson/proj/twitter/twitter_access_token_secret.txt') as f:
    access_token_secret = f.read().strip()

from twython import Twython

twitter = Twython(consumer_key,consumer_secret,access_token,access_token_secret)
```]

---
# Example: Twitter

.smallest[
```python
public_tweets = twitter.search(q='columbia')['statuses']
for status in public_tweets[:3]:
    print('-------')
    print(status["text"])
```
```
-------
RT @NWRiverPartners: Find it difficult to keep track of all of 
the fish and wildlife efforts being made in the Columbia River basin? 
The ne…
-------
RT @TrailerTodd: 18 Wheeler - Big Rig 2008 Freightliner Columbia Tandem
Daycab Tractor Truck #7442 https://t.co/1GtX7bj6QC Semi Truck For S…
-------
Why Columbia, then Franklin? The supply-bearing Nashville and Decatur 
Railroad ran through Pulaski and Columbia, as… https://t.co/hseY9KqnuO
-------
RT @kathy_perez06: Everyone please be careful around Columbia Pike in 
Arlington, VA.
-------
I swear to god working at Columbia Uni must rot one's brain
```]

---
# T-Step: Transform

- Standardization
- Creating dummy variables
- Filling missing
- One-Hot-Encoding
- Binning
- Parsing natural language
- Dimensionality reduction
- etc...

---
# Transform: Pipeline Example 1

```python
from sklearn.svm import SVC
from sklearn.datasets import samples_generator
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
from sklearn.pipeline import Pipeline

# generate some data to play with
X, y = samples_generator.make_classification(n_informative=5,
                                             n_redundant=0,
                                             random_state=42)
X.shape
```
```
(100,20)
```

.tiny[
https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html]


---
# Transform: Pipeline Example 1

.smallest[
```python
feature_filter = SelectKBest(f_regression, k=5)
clf = SVC(kernel='linear')

pipeline = Pipeline([('select', feature_filter), ('svc', clf)])

pipeline.set_params(select__k=10, svc__C=.1).fit(X, y)
```
```
Pipeline(memory=None,
         steps=[('select',
                 SelectKBest(k=10,
                             score_func=&lt;function f_regression at 0x7fad4a96b050&gt;)),
                ('svc',
                 SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,
                     ...
                     tol=0.001, verbose=False))],
         verbose=False)
```]

--
count:false
.smallest[
```python
pipeline.score(X,y)
```
```
0.83
```]

--
count:false
.smallest[
```python
pipeline['select'].get_support()
```
```
array([False, False,  True,  ..., False, False])
```]


---
# Transform: Pipeline Example 2

.smallest[
```python
*from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV

np.random.seed(0)

# Read data from Titanic dataset.
titanic_url = ('https://raw.githubusercontent.com/amueller/'
               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')
data = pd.read_csv(titanic_url)

# Numeric Features:
# - age: float.
# - fare: float.
# Categorical Features:
# - embarked: categories encoded as strings {'C', 'S', 'Q'}.
# - sex: categories encoded as strings {'female', 'male'}.
# - pclass: ordinal integers {1, 2, 3}.
```]

.tiny[
https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py]


---
# Transform: Pipeline Example 2

--
count:false
.smaller[
```python
numeric_features = ['age', 'fare']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])
```]

--
count:false
.smaller[
```python
categorical_features = ['embarked', 'sex', 'pclass']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
```]

--
count:false
.smaller[
```python
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])
```]

--
count:false
.smaller[
```python
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', LogisticRegression(solver='lbfgs'))])
```]

---
# Transform: Pipeline Example 2

.smaller[
```python
X = data.drop('survived', axis=1)
y = data['survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf.fit(X_train, y_train)
print("model score: {:.3f}".format(clf.score(X_test, y_test)))
```
```
model score: 0.775
```]

---
# Transform: Pipeline Example 2

.smallest[
```python
# grid search deep inside the pipeline
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean', 'median'],
    'classifier__C': [0.1, 1.0, 10, 100],
}
```]

--
count:false
.smallest[
```python
grid_search = GridSearchCV(clf, param_grid, cv=10)
grid_search.fit(X_train, y_train)

print("best logistic regression from grid search: {:.3f}".format(grid_search.score(X_test, y_test)))
print("best parameter settings: {}".format(grid_search.best_params_))
```
```
best logistic regression from grid search: 0.798
best parameter settings: {'classifier__C': 0.1, 'preprocessor__num__imputer__strategy': 'mean'}
```]

---
# L-Step: Load

- to_csv
- to excel
- to_json
- to_html
- to_parquet
- to_sql
- to_clipboard
- to_pickle

---
class:middle

# Questions?


---
# Creating APIs: Flask

- return transformed data
- return predictions
- return datasets
- ...


- Flask : lightweight web server

---
# Creating APIs: Flask

.smallest[
```python
# create a file hell_flask.py
from flask import Flask, escape, request

app = Flask(__name__)

@app.route('/')
def hello():
    name = request.args.get("name", "World")
    return f'Hello, {escape(name)}!'

if __name__ == '__main__':
    app.run()
```]

--
count:false
.smallest[
```
# at the command line, run the file using python
$ python hello_flask.py
```]
--
count:false
.smallest[
```python
# in ipython (or notebook)
import requests
r = requests.get('http://127.0.0.1:5000/?name=Bryan')
r.text
```
```
'Hello, Bryan!'
```]

---
# Creating APIs: Flask

.smaller[
```python
import numpy as np
from flask import Flask, request, jsonify
app = Flask(__name__)

@app.route("/")
def help():
    return "Give the number of sides the die should have.\n"

@app.route("/&lt;int:sides&gt;")
def roll_die(sides):
    return str(np.random.randint(1,sides+1))+'\n'

@app.route("/json/&lt;int:sides&gt;")
def roll_die_json(sides):
    return jsonify({'sides': sides,'roll': np.random.randint(1,sides+1)})

if __name__ == '__main__':
    app.run()
```]

---
# GET vs POST

--
count:false
- GET: pass information in the url
    - `127.0.0.1:5000/?firstname=Bryan&amp;lastname=Gibson`

--

- POST: pass information as additional http request (often JSON)

    - `127.0.0.1:5000/`
    - `{'firstname':'Bryan','lastname':'Gibson'}`


---
# Creating APIs: Flask

- Export trained models (and other data structures) using pickle

.smallest[
```python
import pickle as pkl

with open('../src/titanic_clf.pkl','wb') as f:
    pkl.dump(grid_search,f)
```]

---
## Creating APIs: Flask

.smallest[
```python
from flask import Flask, escape, request, jsonify
import pickle as pkl
import pandas as pd

# need to train and pickle classifier
with open('./titanic_clf.pkl','rb') as f:
    clf = pkl.load(f)

app = Flask(__name__)

@app.route('/',methods=['POST'])
def predict():
    prediction = None
    req_data = pd.DataFrame(request.form,index=[0])
    print(req_data,flush=True)
    if req_data is not None:
        prediction = clf.predict(req_data)
    if prediction:
        return jsonify([str(x) for x in prediction])
    else:
        return 'no predictions made'

if __name__ == '__main__':
    app.run()

```]


---
# Data Processing Summary


- ETL
- reading datafiles using pandas
- website scraping (requests,BeautifulSoup)
- accessing data via API
- Tranforming data with Pipelines
- Exposing data via API (Flask)


---
class:middle

# Questions?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);

    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
