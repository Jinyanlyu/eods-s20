{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X = pd.DataFrame(data['data'],columns=data['feature_names'])\n",
    "y = pd.Series(data['target'],name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019908 -0.017646  \n",
       "1 -0.039493 -0.068330 -0.092204  \n",
       "2 -0.002592  0.002864 -0.025930  \n",
       "3  0.034309  0.022692 -0.009362  \n",
       "4 -0.002592 -0.031991 -0.046641  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151.0\n",
       "1     75.0\n",
       "2    141.0\n",
       "3    206.0\n",
       "4    135.0\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr training set score: 0.52\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression().fit(X,y)\n",
    "lr_train_score = lr.score(X,y)\n",
    "print(f'lr training set score: {lr_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr cv score: 0.48\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LinearRegression(),X,y,cv=5)\n",
    "lr_cv_score = np.mean(scores)\n",
    "print(f'lr cv score: {lr_cv_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-792.1841616283061,\n",
       " -239.81908936565472,\n",
       " -10.012197817470847,\n",
       " 67.62538639104386,\n",
       " 101.04457032134488,\n",
       " 177.0641762322512,\n",
       " 324.3904276893763,\n",
       " 476.74583782366255,\n",
       " 519.8397867901343,\n",
       " 751.2793210873945]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr training set score: 0.52\n",
      "ridge training set score: 0.45\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge().fit(X,y)\n",
    "ridge_train_score = ridge.score(X,y)\n",
    "print(f'lr training set score: {lr_train_score:0.2f}')\n",
    "print(f'ridge training set score: {ridge_train_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr cv score: 0.48\n",
      "ridge cv score: 0.41\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(Ridge(),X,y,cv=5)\n",
    "ridge_cv_score = np.mean(scores)\n",
    "print(f'lr cv score: {lr_cv_score:0.2f}')\n",
    "print(f'ridge cv score: {ridge_cv_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-152.04046539318358,\n",
       " -83.15488546325011,\n",
       " -29.515926646857444,\n",
       " 5.909368961868377,\n",
       " 29.465745644229376,\n",
       " 111.87871800112734,\n",
       " 117.31171538202865,\n",
       " 201.6294338387023,\n",
       " 262.944995326854,\n",
       " 306.3516270563722]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr training set score: 0.52\n",
      "ridge training set score: 0.45\n",
      "ridgecv training set score: 0.51\n"
     ]
    }
   ],
   "source": [
    "ridgecv = RidgeCV().fit(X,y)\n",
    "ridgecv_train_score = ridgecv.score(X,y)\n",
    "print(f'lr training set score: {lr_train_score:0.2f}')\n",
    "print(f'ridge training set score: {ridge_train_score:0.2f}')\n",
    "print(f'ridgecv training set score: {ridgecv_train_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr cv score: 0.48\n",
      "ridge cv score: 0.41\n",
      "ridgecv cv score: 0.48\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(RidgeCV(),X,y,cv=5)\n",
    "ridgecv_cv_score = np.mean(scores)\n",
    "print(f'lr cv score: {lr_cv_score:0.2f}')\n",
    "print(f'ridge cv score: {ridge_cv_score:0.2f}')\n",
    "print(f'ridgecv cv score: {ridgecv_cv_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=None, fit_intercept=True,\n",
       "        gcv_mode=None, normalize=False, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-207.19481363268872,\n",
       " -188.68016350975154,\n",
       " -83.46607377280492,\n",
       " -70.82809550533162,\n",
       " 1.3073489468086006,\n",
       " 86.74853944271896,\n",
       " 115.71270250414933,\n",
       " 301.76943731602574,\n",
       " 443.8140541197255,\n",
       " 489.69108008500336]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ridgecv.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr training set score: 0.52\n",
      "ridge training set score: 0.45\n",
      "ridgecv training set score: 0.51\n",
      "lasso training set score: 0.36\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso().fit(X,y)\n",
    "lasso_train_score = lasso.score(X,y)\n",
    "print(f'lr training set score: {lr_train_score:0.2f}')\n",
    "print(f'ridge training set score: {ridge_train_score:0.2f}')\n",
    "print(f'ridgecv training set score: {ridgecv_train_score:0.2f}')\n",
    "print(f'lasso training set score: {lasso_train_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr cv score: 0.48\n",
      "ridge cv score: 0.41\n",
      "ridgecv cv score: 0.48\n",
      "lasso cv score: 0.34\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(Lasso(),X,y,cv=5)\n",
    "lasso_cv_score = np.mean(scores)\n",
    "print(f'lr cv score: {lr_cv_score:0.2f}')\n",
    "print(f'ridge cv score: {ridge_cv_score:0.2f}')\n",
    "print(f'ridgecv cv score: {ridgecv_cv_score:0.2f}')\n",
    "print(f'lasso cv score: {lasso_cv_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 6.301904186054684,\n",
       " 307.60570000448166,\n",
       " 367.7018520698162]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr training set score: 0.52\n",
      "ridge training set score: 0.45\n",
      "ridgecv training set score: 0.51\n",
      "lasso training set score: 0.36\n",
      "lassocv training set score: 0.52\n"
     ]
    }
   ],
   "source": [
    "lassocv = LassoCV().fit(X,y)\n",
    "lassocv_train_score = lassocv.score(X,y)\n",
    "print(f'lr training set score: {lr_train_score:0.2f}')\n",
    "print(f'ridge training set score: {ridge_train_score:0.2f}')\n",
    "print(f'ridgecv training set score: {ridgecv_train_score:0.2f}')\n",
    "print(f'lasso training set score: {lasso_train_score:0.2f}')\n",
    "print(f'lassocv training set score: {lassocv_train_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr cv score: 0.48\n",
      "ridge cv score: 0.41\n",
      "ridgecv cv score: 0.48\n",
      "lasso cv score: 0.34\n",
      "lassocv cv score: 0.48\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LassoCV(),X,y,cv=5)\n",
    "lassocv_cv_score = np.mean(scores)\n",
    "print(f'lr cv score: {lr_cv_score:0.2f}')\n",
    "print(f'ridge cv score: {ridge_cv_score:0.2f}')\n",
    "print(f'ridgecv cv score: {ridgecv_cv_score:0.2f}')\n",
    "print(f'lasso cv score: {lasso_cv_score:0.2f}')\n",
    "print(f'lassocv cv score: {lassocv_cv_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "        max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
       "        positive=False, precompute='auto', random_state=None,\n",
       "        selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003753767152692203"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-569.4426837980648,\n",
       " -235.99640533733526,\n",
       " -6.496844548604692,\n",
       " -0.0,\n",
       " 66.83430445374282,\n",
       " 143.6995665012068,\n",
       " 302.45627915380334,\n",
       " 321.06689245006095,\n",
       " 521.738542606749,\n",
       " 669.9263311180067]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lassocv.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr training set score: 0.52\n",
      "ridge training set score: 0.45\n",
      "ridgecv training set score: 0.51\n",
      "lasso training set score: 0.36\n",
      "lassocv training set score: 0.52\n",
      "en training set score: 0.01\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet().fit(X,y)\n",
    "en_train_score = en.score(X,y)\n",
    "print(f'lr training set score: {lr_train_score:0.2f}')\n",
    "print(f'ridge training set score: {ridge_train_score:0.2f}')\n",
    "print(f'ridgecv training set score: {ridgecv_train_score:0.2f}')\n",
    "print(f'lasso training set score: {lasso_train_score:0.2f}')\n",
    "print(f'lassocv training set score: {lassocv_train_score:0.2f}')\n",
    "print(f'en training set score: {en_train_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr cv score: 0.48\n",
      "ridge cv score: 0.41\n",
      "ridgecv cv score: 0.48\n",
      "lasso cv score: 0.34\n",
      "lassocv cv score: 0.48\n",
      "en cv score: -0.02\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ElasticNet(),X,y,cv=5)\n",
    "en_cv_score = np.mean(scores)\n",
    "print(f'lr cv score: {lr_cv_score:0.2f}')\n",
    "print(f'ridge cv score: {ridge_cv_score:0.2f}')\n",
    "print(f'ridgecv cv score: {ridgecv_cv_score:0.2f}')\n",
    "print(f'lasso cv score: {lasso_cv_score:0.2f}')\n",
    "print(f'lassocv cv score: {lassocv_cv_score:0.2f}')\n",
    "print(f'en cv score: {en_cv_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.86136320233039,\n",
       " 0.0,\n",
       " 0.25093508318279734,\n",
       " 0.3590178701173919,\n",
       " 0.528645560002729,\n",
       " 1.7698509616010372,\n",
       " 2.114454026654008,\n",
       " 2.204355574953443,\n",
       " 3.105840727076851,\n",
       " 3.25976724407584]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(en.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr training set score: 0.52\n",
      "ridge training set score: 0.45\n",
      "ridgecv training set score: 0.51\n",
      "lasso training set score: 0.36\n",
      "lassocv training set score: 0.52\n",
      "en training set score: 0.01\n",
      "encv training set score: 0.52\n"
     ]
    }
   ],
   "source": [
    "encv = ElasticNetCV(l1_ratio=[.001,.5,1]).fit(X,y)\n",
    "encv_train_score = encv.score(X,y)\n",
    "print(f'lr training set score: {lr_train_score:0.2f}')\n",
    "print(f'ridge training set score: {ridge_train_score:0.2f}')\n",
    "print(f'ridgecv training set score: {ridgecv_train_score:0.2f}')\n",
    "print(f'lasso training set score: {lasso_train_score:0.2f}')\n",
    "print(f'lassocv training set score: {lassocv_train_score:0.2f}')\n",
    "print(f'en training set score: {en_train_score:0.2f}')\n",
    "print(f'encv training set score: {encv_train_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr cv score: 0.48\n",
      "ridge cv score: 0.41\n",
      "ridgecv cv score: 0.48\n",
      "lasso cv score: 0.34\n",
      "lassocv cv score: 0.48\n",
      "en cv score: -0.02\n",
      "encv cv score: 0.48\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ElasticNetCV(l1_ratio=[.001,.5,1]),X,y,cv=5)\n",
    "encv_cv_score = np.mean(scores)\n",
    "print(f'lr cv score: {lr_cv_score:0.2f}')\n",
    "print(f'ridge cv score: {ridge_cv_score:0.2f}')\n",
    "print(f'ridgecv cv score: {ridgecv_cv_score:0.2f}')\n",
    "print(f'lasso cv score: {lasso_cv_score:0.2f}')\n",
    "print(f'lassocv cv score: {lassocv_cv_score:0.2f}')\n",
    "print(f'en cv score: {en_cv_score:0.2f}')\n",
    "print(f'encv cv score: {encv_cv_score:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "             l1_ratio=[0.001, 0.5, 1], max_iter=1000, n_alphas=100, n_jobs=None,\n",
       "             normalize=False, positive=False, precompute='auto',\n",
       "             random_state=None, selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003753767152692203"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encv.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-569.4426837980648,\n",
       " -235.99640533733526,\n",
       " -6.496844548604692,\n",
       " -0.0,\n",
       " 66.83430445374282,\n",
       " 143.6995665012068,\n",
       " 302.45627915380334,\n",
       " 321.06689245006095,\n",
       " 521.738542606749,\n",
       " 669.9263311180067]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(encv.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l2'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=0.5, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='elasticnet',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(penalty='elasticnet',l1_ratio=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LogisticRegression in module sklearn.linear_model._logistic object:\n",
      "\n",
      "class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin)\n",
      " |  LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data.  If int, random_state is the seed used by the random number\n",
      " |      generator; If RandomState instance, random_state is the random number\n",
      " |      generator; If None, the random number generator is the RandomState\n",
      " |      instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
      " |      'liblinear'.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not support setting ``penalty='none'``\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'`. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-s20",
   "language": "python",
   "name": "eods-s20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
